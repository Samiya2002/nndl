{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjf1bX4TljMU",
        "outputId": "197c7d90-68e8-48fb-d1d2-d57a3a9aa85c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2110848/2110848 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "253/253 [==============================] - 31s 108ms/step - loss: 2.2913 - accuracy: 0.4090 - val_loss: 2.0173 - val_accuracy: 0.4861\n",
            "Epoch 2/5\n",
            "253/253 [==============================] - 26s 102ms/step - loss: 1.7969 - accuracy: 0.5251 - val_loss: 1.7651 - val_accuracy: 0.5551\n",
            "Epoch 3/5\n",
            "253/253 [==============================] - 31s 122ms/step - loss: 1.6084 - accuracy: 0.5734 - val_loss: 1.6927 - val_accuracy: 0.5584\n",
            "Epoch 4/5\n",
            "253/253 [==============================] - 27s 105ms/step - loss: 1.4011 - accuracy: 0.6223 - val_loss: 1.5408 - val_accuracy: 0.6085\n",
            "Epoch 5/5\n",
            "253/253 [==============================] - 28s 109ms/step - loss: 1.1528 - accuracy: 0.6902 - val_loss: 1.4279 - val_accuracy: 0.6485\n",
            "71/71 [==============================] - 2s 23ms/step - loss: 1.4757 - accuracy: 0.6420\n",
            "Test loss: 1.4757, accuracy: 0.6420\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import reuters\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load the Reuters dataset\n",
        "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "maxlen = 100  # Set a maximum length for the input sequences\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# One-hot encode the labels\n",
        "num_classes = max(y_train) + 1\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=100, input_length=maxlen))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test loss: {loss:.4f}, accuracy: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aUwzdkUslycF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}